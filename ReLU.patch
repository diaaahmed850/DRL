--- /home/abdozizo111/.virtualenvs/dl4cv/lib/python3.6/site-packages/torch/nn/modules/activation.py
+++ /home/abdozizo111/.virtualenvs/dl4cv/lib/python3.6/site-packages/torch/nn/modules/activation.py
@@ -1,8 +1,7 @@
-class ReLU(Threshold):
-    r"""Applies the rectified linear unit function element-wise
+class ReLU(Module):
+    r"""Applies the rectified linear unit function element-wise:
+
     :math:`\text{ReLU}(x)= \max(0, x)`
-
-    .. image:: scripts/activation_images/ReLU.png
 
     Args:
         inplace: can optionally do the operation in-place. Default: ``False``
@@ -12,15 +11,30 @@
           dimensions
         - Output: :math:`(N, *)`, same shape as the input
 
+    .. image:: scripts/activation_images/ReLU.png
+
     Examples::
 
         >>> m = nn.ReLU()
         >>> input = torch.randn(2)
         >>> output = m(input)
+
+
+      An implementation of CReLU - https://arxiv.org/abs/1603.05201
+
+        >>> m = nn.ReLU()
+        >>> input = torch.randn(2).unsqueeze(0)
+        >>> output = torch.cat((m(input),m(-input)))
     """
+    __constants__ = ['inplace']
 
     def __init__(self, inplace=False):
-        super(ReLU, self).__init__(0., 0., inplace)
+        super(ReLU, self).__init__()
+        self.inplace = inplace
+
+    @weak_script_method
+    def forward(self, input):
+        return F.relu(input, inplace=self.inplace)
 
     def extra_repr(self):
         inplace_str = 'inplace' if self.inplace else ''